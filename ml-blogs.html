<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ML Research & Papers - Mukul Ranjan">
    <title>ML Research & Papers - Mukul Ranjan</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="blog-style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Navigation -->
        <nav class="blog-nav">
            <a href="index.html" class="back-link">← Back to Home</a>
            <div class="blog-links">
                <a href="ml-blogs.html" class="active">ML Research</a>
                <a href="personal.html">Personal</a>
                <a href="misc.html">Miscellaneous</a>
            </div>
        </nav>

        <!-- Header -->
        <header class="blog-header">
            <h1>ML Research & Papers</h1>
            <p>Deep dives into papers, research ideas, and machine learning concepts</p>
        </header>

        <!-- Blog Posts -->
        <section class="blog-posts">
            <!-- Example Blog Post Template -->
            <article class="blog-post">
                <div class="post-header">
                    <h2><a href="posts/elastic-cache-explained.html">Understanding Elastic-Cache: Efficient KV Cache for Diffusion LLMs</a></h2>
                    <div class="post-meta">
                        <span class="date">January 2026</span>
                        <span class="reading-time">8 min read</span>
                    </div>
                </div>
                <p class="post-excerpt">
                    A detailed explanation of our work on accelerating diffusion language models
                    using attention-aware KV cache management. We achieve 45x speedups without sacrificing accuracy.
                </p>
                <div class="post-tags">
                    <span class="tag">Diffusion Models</span>
                    <span class="tag">Efficiency</span>
                    <span class="tag">Inference</span>
                </div>
            </article>

            <article class="blog-post">
                <div class="post-header">
                    <h2><a href="posts/temporal-reasoning-vlms.html">Why Vision-Language Models Fail at Temporal Reasoning</a></h2>
                    <div class="post-meta">
                        <span class="date">December 2025</span>
                        <span class="reading-time">10 min read</span>
                    </div>
                </div>
                <p class="post-excerpt">
                    Our SpookyBench reveals a fundamental limitation: while humans achieve 98% accuracy on temporal
                    patterns, state-of-the-art VLMs score 0%. Here's why and what it means for video understanding.
                </p>
                <div class="post-tags">
                    <span class="tag">Vision-Language Models</span>
                    <span class="tag">Temporal Reasoning</span>
                    <span class="tag">Benchmarking</span>
                </div>
            </article>

            <article class="blog-post">
                <div class="post-header">
                    <h2><a href="posts/gradient-based-pruning.html">Gradient-Based Pruning: 634x Faster Than Weight Updates</a></h2>
                    <div class="post-meta">
                        <span class="date">November 2025</span>
                        <span class="reading-time">12 min read</span>
                    </div>
                </div>
                <p class="post-excerpt">
                    GBLM-Pruner shows that gradients alone can guide pruning decisions without expensive weight updates.
                    We explore the theory and practice behind this approach and its implications for LLM compression.
                </p>
                <div class="post-tags">
                    <span class="tag">Pruning</span>
                    <span class="tag">Sparsity</span>
                    <span class="tag">LLMs</span>
                </div>
            </article>

            <article class="blog-post">
                <div class="post-header">
                    <h2><a href="posts/hardware-aware-ml.html">Hardware-Aware ML: Lessons from Hybrid Models on AMD NPUs</a></h2>
                    <div class="post-meta">
                        <span class="date">October 2025</span>
                        <span class="reading-time">15 min read</span>
                    </div>
                </div>
                <p class="post-excerpt">
                    Reflections from implementing heterogeneous inference for Hymba and Jamba on AMD Ryzen AI SoCs.
                    How hardware constraints shape algorithmic decisions and vice versa.
                </p>
                <div class="post-tags">
                    <span class="tag">Hardware-Software Co-Design</span>
                    <span class="tag">NPU</span>
                    <span class="tag">Edge Inference</span>
                </div>
            </article>

            <article class="blog-post">
                <div class="post-header">
                    <h2><a href="posts/parameter-efficient-finetuning.html">GLoRA: Unifying Parameter-Efficient Fine-Tuning Methods</a></h2>
                    <div class="post-meta">
                        <span class="date">September 2025</span>
                        <span class="reading-time">11 min read</span>
                    </div>
                </div>
                <p class="post-excerpt">
                    How we unified LoRA, VPT, Adapters, and SSF under a single mathematical framework and used
                    evolutionary search to find optimal per-layer structures with zero inference overhead.
                </p>
                <div class="post-tags">
                    <span class="tag">PEFT</span>
                    <span class="tag">Fine-Tuning</span>
                    <span class="tag">LoRA</span>
                </div>
            </article>

            <!-- Placeholder for future posts -->
            <!-- <div class="coming-soon">
                <p>More posts coming soon...</p>
                <p class="hint">Subscribe via <a href="#">RSS</a> to stay updated.</p>
            </div> -->
        </section>

        <!-- Footer -->
        <footer>
            <p>© 2026 Mukul Ranjan</p>
        </footer>
    </div>
</body>
</html>
