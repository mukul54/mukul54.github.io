<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Research & Papers - Mukul Ranjan</title>
    <style>
        body {
            font-family: 'Georgia', serif;
            font-size: 18px;
            line-height: 1.6;
            color: #333;
            max-width: 700px;
            margin: 60px auto;
            padding: 0 20px;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 400;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            color: #666;
            margin-bottom: 60px;
        }

        .back-link {
            color: #666;
            text-decoration: none;
            font-size: 0.9em;
            margin-bottom: 40px;
            display: inline-block;
        }

        .back-link:hover {
            color: #333;
        }

        .blog-post {
            margin-bottom: 50px;
        }

        .blog-post h2 {
            font-size: 1.5em;
            font-weight: 500;
            margin: 0 0 8px 0;
        }

        .blog-post h2 a {
            color: #333;
            text-decoration: none;
        }

        .blog-post h2 a:hover {
            color: #2a7ae2;
        }

        .post-meta {
            font-size: 0.9em;
            color: #888;
            margin-bottom: 12px;
        }

        .post-excerpt {
            color: #555;
            line-height: 1.6;
        }

        .coming-soon {
            opacity: 0.5;
        }

        .coming-soon h2 {
            color: #666;
        }

        .tag-label {
            font-size: 0.75em;
            color: #999;
            margin-left: 8px;
            font-weight: normal;
        }
    </style>
</head>
<body>
    <a href="index.html" class="back-link">← Back to Home</a>

    <h1>ML Research & Papers</h1>
    <p class="subtitle">Deep dives into papers, research ideas, and machine learning concepts</p>

    <div class="blog-post">
        <h2><a href="blogs/pytorch-register-buffer/index.html">Register Buffer and Register Parameter in PyTorch</a></h2>
        <div class="post-meta">January 2026 · 8 min read</div>
        <p class="post-excerpt">
            Register Buffer and Register Parameter in PyTorch
        </p>
    </div>

    <div class="blog-post">
        <h2><a href="blogs/pruning-llm/index.html">Understanding the Pruning of Large Language Models</a></h2>
        <div class="post-meta">January 2026 · 25 min read</div>
        <p class="post-excerpt">
            A comprehensive deep dive into neural network pruning fundamentals. From Optimal Brain Damage to SparseGPT,
            exploring the mathematics, algorithms, and practical techniques for compressing LLMs by 2-4x while maintaining performance.
        </p>
    </div>

    <div class="blog-post coming-soon">
        <h2>Why Vision-Language Models Fail at Temporal Reasoning<span class="tag-label">[Coming Soon]</span></h2>
        <div class="post-meta">December 2025 · 10 min read</div>
        <p class="post-excerpt">
            Our SpookyBench reveals a fundamental limitation: while humans achieve 98% accuracy on temporal
            patterns, state-of-the-art VLMs score 0%. Here's why and what it means for video understanding.
        </p>
    </div>

    <div class="blog-post coming-soon">
        <h2>Gradient-Based Pruning: Does Gradient Help in LLM Pruning?<span class="tag-label">[WIP]</span></h2>
        <div class="post-meta">November 2025 · 12 min read</div>
        <p class="post-excerpt">
            GBLM-Pruner shows that gradients can guide pruning decisions without expensive weight updates.
            We explore the theory and practice behind this approach and its implications for LLM compression.
        </p>
    </div>

    <div class="blog-post coming-soon">
        <h2>Hardware-Aware ML: Lessons from Hybrid Models on AMD NPUs<span class="tag-label">[Coming Soon]</span></h2>
        <div class="post-meta">October 2025 · 15 min read</div>
        <p class="post-excerpt">
            Reflections from implementing heterogeneous inference for Hymba and Jamba on AMD Ryzen AI SoCs.
            How hardware constraints shape algorithmic decisions and vice versa.
        </p>
    </div>

    <div class="blog-post coming-soon">
        <h2>GLoRA: Unifying Parameter-Efficient Fine-Tuning Methods<span class="tag-label">[WIP]</span></h2>
        <div class="post-meta">September 2025 · 11 min read</div>
        <p class="post-excerpt">
            How we unified LoRA, VPT, Adapters, and SSF under a single mathematical framework and used
            evolutionary search to find optimal per-layer structures with zero inference overhead.
        </p>
    </div>

    <script data-goatcounter="https://mukulranjan.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>
</html>