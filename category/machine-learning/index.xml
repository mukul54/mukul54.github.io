<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Mukul&#39;s Blog</title>
    <link>https://mukul54.github.io/category/machine-learning/</link>
      <atom:link href="https://mukul54.github.io/category/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 14 May 2023 21:13:14 -0500</lastBuildDate>
    <image>
      <url>https://mukul54.github.io/media/icon_hu090309fb434628b5bc5d7c09ca55ab4a_11217_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>https://mukul54.github.io/category/machine-learning/</link>
    </image>
    
    <item>
      <title>How does Lasso regression(L1) encourage zero coefficients but not the L2?</title>
      <link>https://mukul54.github.io/post/l1_vs_l2/</link>
      <pubDate>Sun, 14 May 2023 21:13:14 -0500</pubDate>
      <guid>https://mukul54.github.io/post/l1_vs_l2/</guid>
      <description>&lt;p&gt;We often read almost everywhere that Lasso regression **encourages zero coefficient, **and hence provide a great tool for &lt;em&gt;variable selection&lt;/em&gt; as well but it is really difficult to get the intuition about this. In this article, I have tried to discuss this in detail.&lt;/p&gt;
&lt;h2 id=&#34;contents&#34;&gt;Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Overfitting and Regularization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intuition 1: Optimize a single coefficient model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intuition 2: Look at this simple example&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intuition 3: Observe this beautiful image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intuition 4: Probabilistic Interpretation of L1 and L2&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;overfitting-and-regularization&#34;&gt;Overfitting and Regularization&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; is a phenomenon where a machine learning model is unable to generalize well on the unseen data. When our model is complex(for example polynomial regression with a very high degree or a very deep neural network) and we have less training data, in those cases model tends to memorize the training data and does not generalize well on unseen data.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*nF6MZQJ431vv-0AGKAj30g.png&#34; alt=&#34;The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line. — &amp;lt;a href=&amp;#34;https://en.wikipedia.org/wiki/Overfitting&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;Wikipedia&amp;lt;/a&amp;gt;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Look at this image from Wikipedia in which the green line shows the decision boundary of the overfitted classifier while the black one shows the regularized one. We see that even though the green decision boundary seems to give no training error it won’t generalize well on the unseen data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularization&lt;/strong&gt; is one of the ways to reduce the overfitting of a machine learning model by adding the extra penalty to the loss function. The penalty is added in terms of some norms of the parameters. When the loss function of the linear regression model uses the L1 norm of the parameters, the regression model is called &lt;strong&gt;Lasso Regression&lt;/strong&gt; while the one which uses the L2 norms is called &lt;strong&gt;Ridge Regression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;intuition-1-optimize-a-single-coefficient-model&#34;&gt;Intuition 1: Optimize a single coefficient model&lt;/h2&gt;
&lt;p&gt;As explained &lt;a href=&#34;https://stats.stackexchange.com/a/368426/211187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, consider a Ridge Regression model with a &lt;em&gt;single coefficient β&lt;/em&gt;, the equation for the loss function of L2 regression in this can be given as follows:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*b_3kerwX5t7Fa9cZKDSLPA.png&#34; alt=&#34;Ridge Regression for single parameter β&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;To minimize this equation, we will have to take the derivative w.r.t β and equate it to 0 to get the coefficient&amp;rsquo;s optimal value.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*xe5u6ZMUB-yIvEf2qtdezQ.png&#34; alt=&#34;Optimizing Ridge Regression for β&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We see from the above equation that for coefficient β to be 0 for non-zero values of x and y, λ→∞. Now let’s look at the case for L1 or lasso regression.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*eO7D5Sa5v0IE2giCAGy70A.png&#34; alt=&#34;Lasso Regression for single coefficient β&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Consider the case where β&amp;gt;0, and minimize the expression for the L1 loss by differentiating it w.r.t β.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*9MKGDXL_zDeTG6spDsJN9Q.png&#34; alt=&#34;Optimizing lasso regression for β&amp;amp;gt;0&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Similarly, for β&amp;lt;0, we get the following equation:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*YJkH5BYbs5KMO9FvLfz3Mw.png&#34; alt=&#34;Optimizing lasso regression for β&amp;amp;lt;0&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;From both of the above equations, we see that in the case of L1 regularization, there are infinite possible values of x and y for a given *λ, *for which it is possible for β to be 0. Hence in contrast to Ridge regression, LASSO or L1 Regression encourages 0 coefficients therefore acting as a method of variable selection.&lt;/p&gt;
&lt;h2 id=&#34;intuition-2-look-at-this-simple-example&#34;&gt;Intuition 2: Look at this simple example&lt;/h2&gt;
&lt;p&gt;This was the first good intuition that I found related to this topic in Murphy’s, &lt;em&gt;&lt;strong&gt;Machine Learning: A Probabilistic Perspective (page no. 431)&lt;/strong&gt;&lt;/em&gt;. Consider a set of sparse vector β with two values, β₁ = (1, 0) and another set of dense vector β with two values such as β₂ = (1/√2, 1/√2).&lt;/p&gt;
&lt;p&gt;In the case of L2 regularization, β₁ and β₂ both assign the same have weight since the L2 norm of both of them is the same.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*mLrF9tv6wbqRCBCMiGIT7Q.png&#34; alt=&#34;L2 norm is the same for sparse and dense feature vector&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But when we look into the case of L1 regularization, if we look at the L1 norm of the β₁(the sparse vector), we find that it is less than that of β₂(the dense vector) as seen in the following equation.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*3wn17B8bk65b8mDrDGzV1g.png&#34; alt=&#34;L1 Norm is less for sparse vector as compared to that of the dense one&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Hence, this shows that LASSO encourages zero coefficients.&lt;/p&gt;
&lt;h2 id=&#34;intuition-3-observe-this-beautiful-image&#34;&gt;Intuition 3: Observe this beautiful image&lt;/h2&gt;
&lt;p&gt;Here, we will look at the famous regularization diagram from Hastie’s &lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESL&lt;/a&gt;’s, page no 71.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*VyjsgGMEmJfqxc0kcDaCYA.png&#34; alt=&#34;Illustration of L1 and L2 regularization (ESL: page 71)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I had a really hard time understanding this figure until I came across &lt;a href=&#34;https://explained.ai/regularization/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; wonderful blog by &lt;a href=&#34;https://explained.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explained.ai&lt;/a&gt;. I highly recommend you to look into that and various other blogs from the same author available at the &lt;a href=&#34;http://explained.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site&lt;/a&gt; as well. They are really much more intuitive and well explained. You can find there the code for all the curves used by them as well.&lt;/p&gt;
&lt;p&gt;Let’s look at the following two diagrams from the above-mentioned blogs.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*gGb-vPk2sctWXdhRHGAYrA.png&#34; alt=&#34;Remember that elliptical curves here are the curves for unconstrained cost function i.e. without any addition of L1 or L2 norms of the coefficients. The black dot at the center of the elliptical curve is the point where the value of cost function is 0 and as we move away from that black dot, its value increases, so higher cost curves are farther from the black dot(Source: &amp;lt;a href=&amp;#34;https://explained.ai/regularization/L1vsL2.html&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;explained.ai&amp;lt;/a&amp;gt;).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We see that the minimum cost in the case of the L1 is given by the &lt;strong&gt;purple dot&lt;/strong&gt; at the diamond tip. As we move on the edge of the diamond, we find ourselves to be moving away from the **black dot **and hence there is a higher cost associated with it, for example, look at the &lt;strong&gt;yellow dot&lt;/strong&gt; on the edge of the diamond. Hence in the case of L1 or LASSO regression, it is more likely to find the optimal parameter values at the tip of the diamond. In contrast to this, let’s look at the case of Ridge regression, i.e the L2 constrained circle; we see that the optimal value of parameters is not on the axis since we get the minimum cost at the purple dot, which is away from the axis. To be more clear, let’s look at another figure from the same blog.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*HmpYrhbz_rivr0cmSSvxLg.png&#34; alt=&#34;Zoomed In figure showing the optimal parameters for L1 and L2 regression at the purple point. Moving away from the L1 diamond point immediately increases loss, but L2 can move a little bit upwards before moving leftward away from the loss function minimum. (Source: &amp;lt;a href=&amp;#34;https://explained.ai/regularization/L1vsL2.html&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;explained.ai&amp;lt;/a&amp;gt;)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;intuition-4-probabilistic-interpretation-of-l1-and-l2&#34;&gt;Intuition 4: Probabilistic Interpretation of L1 and L2&lt;/h2&gt;
&lt;p&gt;For this part, I assume that you know some basics of the Bayes Theorem. You can look into some of the resources related to it. I will skip a lot of details here. For more details, you can look into the answers to &lt;a href=&#34;https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; cross-validated question and this wonderful &lt;a href=&#34;http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt; by Brian Keng. Most of the equations and explanations here are from &lt;em&gt;Brian Keng’s blog.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Maximum log-likelihood estimate for a linear regression model can be given by&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*QOVksPGNCEXfayeK-iVeEw.png&#34; alt=&#34;Maximum Likelihood estimate for an ordinary Linear Regression Model(For more details look at this &amp;lt;a href=&amp;#34;http://bjlkeng.github.io/posts/a-probabilistic-view-of-regression/&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;blog&amp;lt;/a&amp;gt;)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We simply choose that β for which mean squared error between the observed value y and predicted value ŷ is minimum. With a simple modification to the above expression, the maximum log-likelihood estimate for L1 and L2 regression can be written as follows:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*_Sh9Z5mPAoDinwfkvn-f4A.png&#34; alt=&#34;Maximum Likelihood Estimate for L1 and L2 regression&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Likelihood estimate for ordinary linear regression can also be given by following(when we do not consider log) equation:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*w_l0fIeAjViQVXZNw6tLnQ.png&#34; alt=&#34;The likelihood function of Ordinary Linear Regression&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayesian_inference#Formal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayes Theorem&lt;/a&gt; we know that the &lt;strong&gt;posterior&lt;/strong&gt;, is defined as follows:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*MpWu20bisKBZsqqGSTB2Zg.png&#34; alt=&#34;Bayes Theorem&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In case of Bayesian methods we are primarily concerned about the &lt;strong&gt;Posterior&lt;/strong&gt;, i.e. the probability distribution of parameter β given the observed data y in contrast to the classical methods where we try to find the best parameters to maximize the &lt;strong&gt;likelihood&lt;/strong&gt; i.e. the probability of observing data(y) given different value of parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Priors are simply some additional previous information about β before coming across the data y.&lt;/p&gt;
&lt;h3 id=&#34;the-maximum-a-posteriori-probability-estimatemap&#34;&gt;The maximum a posteriori probability estimate(MAP)&lt;/h3&gt;
&lt;p&gt;In this case, we will try to maximize the P(β|y), i.e. the posterior probability. MAP is closely related to the MLE, but also includes prior distribution, therefore it acts as a regularization of MLE.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*-rQ7GkXJ2V19xP4ro8WKUQ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;l2-regularization-and-gaussian-prior&#34;&gt;L2 Regularization and Gaussian Prior&lt;/h3&gt;
&lt;p&gt;Consider **a zero-mean normally distributed prior on each βᵢ value, all with identical variance τ². From the likelihood equation for Ordinary Linear Regression and the MAP estimate equation which we drive earlier, we have:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*hdXdIKjJ-nfkprfLIt4nfw.png&#34; alt=&#34;L2 Regularization as the MAP estimate of Linear Regression coefficients with Gaussian priors&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Thus, we see that the MAP estimate of Linear Regression coefficients with Gaussian priors gives us L2 or Ridge Regression.&lt;/p&gt;
&lt;p&gt;Note that λ=σ²/τ² in the above equations. Also, remember that σ is assumed to be constant in linear regression, and we get to pick τ for our prior. We can adjust the amount of regularization we want, by changing λ.&lt;/p&gt;
&lt;h3 id=&#34;l1-regularization-and-laplacian-prior&#34;&gt;L1 Regularization and Laplacian Prior&lt;/h3&gt;
&lt;p&gt;The probability distribution function for Laplace distribution is given by the following equation:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*LU17dFXtruHEIh0pnKXYHA.png&#34; alt=&#34;Laplace Distribution&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Considering the zero mean Laplacian priors on all the coefficients as we did in the previous section, we have:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/1*T-knEMUcsuy0Ze9Axr8wxg.png&#34; alt=&#34;L1 Regularization as a MAP of Linear Regression Coefficients with Laplacian priors&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Again, we see that MAP of Linear Regression coefficients with Laplacian priors gives us L1 or Lasso Regression.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/2000/0*17UpawN2lk1tN1ff.png&#34; alt=&#34;Laplace and Gaussian Distribution(source: &amp;lt;a href=&amp;#34;https://stats.stackexchange.com/a/430198/211187&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;Cross Validated&amp;lt;/a&amp;gt;)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Look at the above graph for the Gaussian and Laplace distribution. As we discussed earlier that L1 or LASSO regression can be viewed as putting Laplace priors on the weights. Since the Laplace distribution is more concentrated around zero, our weight is more likely to be zero in the case of L1 or LASSO regularization.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;L1 or LASSO(Least Absolute Shrinkage and Selection Operator) Regularization supports both variable selection and regularization simultaneously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both L1 and L2 regularization problems can be solved using the Lagrangian method of constrained optimization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The lasso penalty will force some of the coefficients quickly to zero. This means that variables are removed from the model, hence the sparsity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ridge regression will more or less compress the coefficients to become smaller. This does not necessarily result in 0 coefficients and the removal of variables.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intuitions on L1 and L2 Regularization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/4961/what-is-regularization-in-plain-english&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is regularization in plain English?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/QNxNCgtWSaY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L1 and L2 Regularization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://explained.ai/regularization/L1vsL2.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The difference between L1 and L2 Regularization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why does the lasso provide variable selection?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hastie, Tibshirani and Friedman, The Elements of Statistical Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Learning: A Probabilistic Perspective, Kevin P. Murphy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/176599/why-will-ridge-regression-not-shrink-some-coefficients-to-zero-like-lasso&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why will ridge regression not shrink some coefficients to zero like lasso&lt;/a&gt;?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why is the L2 regularization equivalent to Gaussian prior?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Probabilistic Interpretation of Regularization&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Overfitting — Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
